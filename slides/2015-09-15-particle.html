---
layout: slide
title: CS 5220
description: Parallelism and locality in simulation
theme: simple
audio: http://www.cs.cornell.edu/~bindel/audio/2015-09-15-particle/
transition: slide
js: js/quiz.js
---

<section data-markdown>
# CS 5220
## Parallelism and locality in simulation
### Particle systems
## 15 Sep 2015
</section>


<section data-markdown>
### Particle simulation

Particles move via Newton ($F = ma$), with

-   External forces: ambient gravity, currents, etc.
-   Local forces: collisions, Van der Waals ($1/r^6$), etc.
-   Far-field forces: gravity and electrostatics ($1/r^2$), etc.
    -   Simple approximations often apply (Saint-Venant)
</section>


<section>
<h3>A forced example</h3>

<p style="text-align:left;">
Example force:
$$f_i = \sum_j Gm_i m_j
      \frac{(x_j-x_i)}{r_{ij}^3}
      \left(1 - \left(\frac{a}{r_{ij}}\right)^{4} \right), \qquad
      r_{ij} = \|x_i-x_j\|$$
</p>

<ul>
  <li>Long-range attractive force ($r^{-2}$)</li>
  <li>Short-range repulsive force ($r^{-6}$)</li>
  <li>Go from attraction to repulsion at radius $a$</li>
</ul>
</section>


<section data-markdown>
### A simple serial simulation

In Matlab, we can write

      npts = 100;
      t = linspace(0, tfinal, npts);
      [tout, xyv] = ode113(@fnbody, ...
                    t, [x; v], [], m, g);
      xout = xyv(:,1:length(x))';

... but I can’t call ode113 in C in parallel (or can I?)
</section>


<section data-markdown>
### A simple serial simulation

Maybe a fixed step leapfrog will do?

      npts = 100;
      steps_per_pt = 10;
      dt = tfinal/(steps_per_pt*(npts-1));
      xout = zeros(2*n, npts);
      xout(:,1) = x;
      for i = 1:npts-1
        for ii = 1:steps_per_pt
          x = x + v*dt;
          a = fnbody(x, m, g);
          v = v + a*dt;
        end
        xout(:,i+1) = x;
      end

</section>


<section data-markdown>
### Pondering particles

-   Where do particles “live” (esp. in distributed memory)?
    -   Decompose in space? By particle number?
    -   What about clumping?
-   How are long-range force computations organized?
-   How are short-range force computations organized?
-   How is force computation load balanced?
-   What are the boundary conditions?
-   How are potential singularities handled?
-   What integrator is used? What step control?
</section>


<section data-markdown>
### External forces

Simplest case: no particle interactions.

-   Embarrassingly parallel (like Monte Carlo)!
-   Could just split particles evenly across processors
-   Is it that easy?
    -   Maybe some trajectories need short time steps?
    -   Even with MC, load balance may not be entirely trivial.
</section>


<section>
<h3>Local forces</h3>

<img width="30%" src="/img/particle/dd1.svg"></img><br/>

<ul>
<li>Simplest all-pairs check is $O(n^2)$ (expensive)</li>
<li>Or only check close pairs (via binning, quadtrees?)</li>
<li>Communication required for pairs checked</li>
<li>Usual model: domain decomposition</li>
</ul>
</section>

<section>
<h3>Local forces: Communication</h3>

<img width="30%" src="/img/particle/dd2.svg"></img><br/>

<p style="text-align: left;">
  Minimize communication:
</p>
<ul>
<li>Send particles that might affect a neighbor <q>soon</q></li>
<li>Trade extra computation against communication</li>
<li>Want low surface area-to-volume ratios on domains</li>
</section>

<section>
<h3>Local forces: Load balance</h3>

<img width="30%" src="/img/particle/dd3.svg"></img><br/>

<ul>
  <li>Are particles evenly distributed?</li>
  <li>Do particles remain evenly distributed?</li>
  <li>Can divide space unevenly (e.g. quadtree/octtree)</li>
</ul>
</section>

<section>
<h3>Far-field forces</h3>

<img width="60%" src="/img/particle/rr1.svg"></img><br/>

<ul>
<li>Every particle affects every other particle</li>
<li>All-to-all communication required</li>
  <ul>
  <li>Overlap communication with computation</li>
  <li>Poor memory scaling if everyone keeps everything!</li>
  </ul>
<li>Idea: pass particles in a round-robin manner</li>
</ul>
</section>

<section>
<h3>Passing particles for far-field forces</h3>

<img width="60%" src="/img/particle/rr1.svg"></img><br/>
<pre><code class="no-highlight" data-trim>
copy local particles to current buf
for phase = 1:p
  send current buf to rank+1 (mod p)
  recv next buf from rank-1 (mod p) 
  interact local particles with current buf
  swap current buf with next buf
end
</code></pre>
</section>

<section>
<h3>Passing particles for far-field forces</h3>

<p style="text-align:left;">
Suppose $n = N/p$ particles in buffer. At each phase
$$
\begin{align}
  t_{\mathrm{comm}} &\approx \alpha + \beta n \\
  t_{\mathrm{comp}} &\approx \gamma n^2
\end{align}
$$
So we can mask communication with computation if
$$
  n \geq \frac{1}{2\gamma} \left( \beta + \sqrt{\beta^2 + 4 \alpha \gamma}
  \right) &gt; \frac{\beta}{\gamma}
$$
</p>

<p style="text-align:left;">
More efficient serial code<br/>
$\implies$ larger $n$ needed to mask communication!<br/>
$\implies$ worse speed-up as $p$ gets larger (fixed $N$)<br/>
but scaled speed-up ($n$ fixed) remains unchanged.
</p>

</section>

<section>
<h3>Far-field forces: particle-mesh methods</h3>

<img width="20%" src="/img/particle/pic1.svg"></img><br/>

<p style="text-align: left;">
Consider $r^{-2}$ electrostatic potential interaction
</p>
<ul>
  <li>Enough charges looks like a continuum!</li>
  <li>Poisson equation maps charge distribution to potential</li>
  <li>Use fast Poisson solvers for regular grids (FFT, multigrid)</li>
  <li>Approximation depends on mesh and particle density</li>
  <li>Can clean up leading part of approximation error</li>
</ul>
</section>

<section>
<h3>Far-field forces: particle-mesh methods</h3>

<img width="20%" src="/img/particle/pic1.svg"></img><br/>

<ul>
  <li>Map particles to mesh points (multiple strategies)</li>
  <li>Solve potential PDE on mesh</li>
  <li>Interpolate potential to particles</li>
  <li>Add correction term – acts like local force</li>
</ul>
</section>

<section>
<h3>Far-field forces: tree methods</h3>

<img width="60%" src="/img/particle/tree.svg"></img><br/>

<ul>
<li>Distance simplifies things
  <ul><li>Andromeda looks like a point mass from here?</li></ul>
</li>
<li>Build a tree, approximating descendants at each node</li>
<li>Several variants: Barnes-Hut, FMM, Anderson’s method</li>
<li>More on this later in the semester</li>
</ul>
</section>

<section data-markdown>
### Summary of particle example

-   Model: Continuous motion of particles
    -   Could be electrons, cars, whatever...
-   Step through discretized time
-   Local interactions
    -   Relatively cheap
    -   Load balance a pain
-   All-pairs interactions
    -   Obvious algorithm is expensive ($O(n^2)$)
    -   Particle-mesh and tree-based algorithms help

An important special case of lumped/ODE models.
</section>
